{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29b9bd1d-766f-4422-ad96-de0accc1ce58"
    }
   },
   "source": [
    "# Lab 1 - Logistic Regression with MNIST\n",
    "\n",
    "\n",
    "# Lab Overview\n",
    "In this lab we will build and train a Multiclass Logistic Regression model using the MNIST data. \n",
    "\n",
    "The lab comprises two parts. During the first part, the instructor will walk you through the code to define, train, and evaluate the initial version of MLR model. In the second part you will compete with other students to improve the performance of the model.\n",
    "\n",
    "The MNIST data consists of hand-written digits with little background noise making it a standard dataset to create, experiment and learn deep learning models with reasonably small comptuing resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\", width=200, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) (LR) is a fundamental machine learning technique that uses a linear weighted combination of features and generates probability-based predictions of different classes.  \n",
    "â€‹\n",
    "There are two basic forms of LR: **Binary LR** (with a single output that can predict two classes) and **multinomial LR** (with multiple outputs, each of which is used to predict a single class).  \n",
    "\n",
    "![LR-forms](http://www.cntk.ai/jup/cntk103b_TwoFormsOfLR-v3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Binary Logistic Regression** (see top of figure above), the input features are each scaled by an associated weight and summed together.  The sum is passed through a squashing (aka activation) function and generates an output in [0,1].  This output value (which can be thought of as a probability) is then compared with a threshold (such as 0.5) to produce a binary label (0 or 1).  This technique supports only classification problems with two output classes, hence the name binary LR.  In the binary LR example shown above, the [sigmoid][] function is used as the squashing function.\n",
    "\n",
    "[sigmoid]: https://en.wikipedia.org/wiki/Sigmoid_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Multiclass Linear Regression** (see bottom of figure above), 2 or more output nodes are used, one for each output class to be predicted.  Each summation node uses its own set of weights to scale the input features and sum them together. Instead of passing the summed output of the weighted input features through a sigmoid squashing function, the output is often passed through a `softmax` function (which in addition to squashing, like the `sigmoid`, the `softmax` normalizes each nodes' output value using the sum of all unnormalized nodes). \n",
    "\n",
    "$$ h(\\textbf{z})_i = \\frac{e^{z_i}}{\\sum_{k=1}^C{e^{z_k}}} \\text{,  where  } \\textbf{z} = \\textbf{W} \\times \\textbf{x}  + \\textbf{b} $$\n",
    "\n",
    "In this lab, we will use multinomial LR for classifying the MNIST digits (0-9) using 10 output nodes (1 for each of our output classes).\n",
    "\n",
    "The figure below summarizes the model in the context of the MNIST data.\n",
    "\n",
    "![mnist-LR](https://www.cntk.ai/jup/cntk103b_MNIST_LR.png)\n",
    "\n",
    "During the model training the values of **W** and **b** parameters are optimized to fit the training samples and generalize well to the samples outside of the training set.\n",
    "\n",
    "This is achieved by iteratively reducing the `loss` function - a function that expresses the \"difference\" between model predictions and training ground-truth labels. \n",
    "\n",
    "`Cross-entropy` is a popular function to measure the loss. It is defined as:\n",
    "\n",
    "$$ H(\\textbf{W},\\textbf{b}) = - \\sum_{j=1}^C y_j \\log (h(\\textbf{z})_j ) \\text{, where }  y_j \\text{ is a ground truth label} $$  \n",
    "\n",
    "Various optimization approaches can be utilized, Stochastic Gradient Descent (`sgd`) being one of the most popular ones. \n",
    "\n",
    "The \"classical\" stochastic gradient descent uses a single observation to calculate gradient estimates and update the model parameters. This approach is  attractive since it does not require the entire data set (all observation) to be loaded in memory and also requires gradient computation over fewer datapoints, thus allowing for training on large data sets. However, the updates generated using a single observation sample at a time can vary wildly between iterations. An intermediate ground is to use a small set of observations and use an average of the `loss` or error from that set to update the model parameters. This subset is called a *minibatch*.\n",
    "\n",
    "With minibatches, we often sample observation from the larger training dataset. We repeat the process of model parameters update using different combination of training samples and over a period of time minimize the `loss` (and the error). When the incremental error rates are no longer changing significantly or after a preset number of maximum minibatches to train, we claim that our model is trained.\n",
    "\n",
    "In the lab we will use the minibatch form of SGD.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "138d1a78-02e2-4bd6-a20e-07b83f303563"
    }
   },
   "outputs": [],
   "source": [
    "# Import the relevant components\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "from cntk.logging.progress_print import ProgressPrinter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "\n",
    "In this tutorial we are using the MNIST data pre-processed to follow CNTK CTF format. \n",
    "\n",
    "\n",
    "    |labels 0 0 0 0 0 0 0 1 0 0 |features 0 0 0 0 ... \n",
    "                                                  (784 integers each representing a pixel)\n",
    "                                                 \n",
    "\n",
    "Each line in the file contains two key-value pairs, also refered as streams. The `labels` stream is the one-hot encoded representation of a digit 0-9. The `features` stream is a 784 vector of 0-255 integers representing 28 x 28 pixel grayscale image.\n",
    "\n",
    "Our dataset includes three files: the training file with 50,000 images, the validation file with 10,000 images, and the testing file with 10,000 images.\n",
    "\n",
    "To read/sample the files, we define a `create_reader` function that configures and returns the CNTK MinibatchSource object.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure we always get the same amount of randomness\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    labelStream = C.io.StreamDef(field='labels', shape=num_label_classes)\n",
    "    featureStream = C.io.StreamDef(field='features', shape=input_dim)\n",
    "    deserializer = C.io.CTFDeserializer(path, C.io.StreamDefs(labels = labelStream, features = featureStream))\n",
    "    return C.io.MinibatchSource(deserializer,\n",
    "       randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition and training\n",
    "In CNTK, a computational network (e.g. a neural network) is a **function object**. On one hand a computational network in CNTK is just a function that you can call to apply to data. On the other hand, a computational network contains learnable parameters that can be accessed like object members. Complicated networks can be composed as hierarchies of simpler ones, which, for example, represent layers. \n",
    "\n",
    "CNTK function objects are represented internally as graph structures in C++ that encode the computation. This graph structure is wraped in the Python class `Function` that exposes the necessary interface so that other Python functions can call it and access its members (such as learnable parameters)\n",
    "\n",
    "The function object is CNTK's single abstraction used to represent different operations from simple **basic operations** without learnable parameters to  **layers**, **recurrent step functions**, **complete models**, **criterion functions** and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the data dimensions\n",
    "input_dim = 784\n",
    "num_output_classes = 10\n",
    "\n",
    "# Create inputs for features and labels\n",
    "features = C.input_variable(input_dim)/255\n",
    "labels = C.input_variable(num_output_classes, is_sparse=True)\n",
    "\n",
    "# Define parameters\n",
    "W = C.parameter(shape=(input_dim, num_output_classes))\n",
    "b = C.parameter(shape=(num_output_classes))\n",
    "\n",
    "# And the network\n",
    "z = C.times(features, W) + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the criterion function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use cross entropy as a loss function\n",
    "loss = C.cross_entropy_with_softmax(z, labels)\n",
    "\n",
    "# And a standard classsification error as an error metric\n",
    "metric = C.classification_error(z, labels)\n",
    "\n",
    "criterion = C.combine([loss, metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Train the model using the SGD learner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an SGD learner\n",
    "learner = C.sgd(z.parameters, C.learning_rate_schedule(0.2, C.UnitType.minibatch))\n",
    "\n",
    "# Define a helper function to report on training progress\n",
    "progress_writer = ProgressPrinter()\n",
    "\n",
    "# Create the reader to the training data set\n",
    "train_file = \"../Data/MNIST_train.txt\"\n",
    "reader_train = create_reader(train_file, True, input_dim, num_output_classes)\n",
    "\n",
    "# Initiate training\n",
    "progress = criterion.train(minibatch_source = reader_train,\n",
    "                    streams = (reader_train.streams.features, reader_train.streams.labels),\n",
    "                    minibatch_size = 64,\n",
    "                    epoch_size = 6400,\n",
    "                    max_epochs = 70,\n",
    "                    parameter_learners=[learner],\n",
    "                    callbacks = [progress_writer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the reader on the validation data set\n",
    "validation_file = \"../Data/MNIST_validate.txt\"\n",
    "reader_validate = create_reader(validation_file, False, input_dim, num_output_classes)\n",
    "\n",
    "# Score the validation set and calculate the classification error metric\n",
    "validation_metric = criterion.test(minibatch_source = reader_validate,\n",
    "                                  minibatch_size = 64,\n",
    "                                  streams = (reader_validate.streams.features, reader_validate.streams.labels),\n",
    "                                  callbacks = [progress_writer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon\n",
    "\n",
    "Try to improve the performance of the model. \n",
    "\n",
    "Hints:\n",
    "- Play with the learning rate, minibatch size and the number of epochs\n",
    "- You can look at regularization - check `l1_regularization` and `l2_regularization` hyper parameters of the `sgd` learner\n",
    "\n",
    "## Final testing\n",
    "\n",
    "\n",
    "DON'T CHEAT. DON'T USE MNIST_test.txt FOR MODEL TRAINING AND SELECTION. DON'T EXECUTE THE BELOW CELL TILL YOU ARE READY FOR THE FINAL TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the reader on the testing data set\n",
    "test_file = '../Data/MNIST_test.txt'\n",
    "reader_test = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "\n",
    "# Score the testing data set and calculate the classification error metric\n",
    "test_metric = criterion.test(minibatch_source = reader_test,\n",
    "                           minibatch_size = 64,\n",
    "                           streams = (reader_test.streams.features, reader_test.streams.labels),\n",
    "                           callbacks = [progress_writer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
