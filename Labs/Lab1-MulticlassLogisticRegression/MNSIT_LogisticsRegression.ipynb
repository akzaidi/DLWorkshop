{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29b9bd1d-766f-4422-ad96-de0accc1ce58"
    }
   },
   "source": [
    "# Lab 2 - Logistic Regression with MNIST\n",
    "\n",
    "\n",
    "# Model Overview\n",
    "In this tutorial we will build and train a Multiclass Logistic Regression model using the MNIST data. \n",
    "\n",
    "The MNIST data comprises of hand-written digits with little background noise making it a standard dataset to create, experiment and learn deep learning models with reasonably small comptuing resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\", width=200, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) (LR) is a fundamental machine learning technique that uses a linear weighted combination of features and generates probability-based predictions of different classes.  \n",
    "â€‹\n",
    "There are two basic forms of LR: **Binary LR** (with a single output that can predict two classes) and **multinomial LR** (with multiple outputs, each of which is used to predict a single class).  \n",
    "\n",
    "![LR-forms](http://www.cntk.ai/jup/cntk103b_TwoFormsOfLR-v3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Binary Logistic Regression** (see top of figure above), the input features are each scaled by an associated weight and summed together.  The sum is passed through a squashing (aka activation) function and generates an output in [0,1].  This output value (which can be thought of as a probability) is then compared with a threshold (such as 0.5) to produce a binary label (0 or 1).  This technique supports only classification problems with two output classes, hence the name binary LR.  In the binary LR example shown above, the [sigmoid][] function is used as the squashing function.\n",
    "\n",
    "[sigmoid]: https://en.wikipedia.org/wiki/Sigmoid_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Multiclass Linear Regression** (see bottom of figure above), 2 or more output nodes are used, one for each output class to be predicted.  Each summation node uses its own set of weights to scale the input features and sum them together. Instead of passing the summed output of the weighted input features through a sigmoid squashing function, the output is often passed through a `softmax` function (which in addition to squashing, like the `sigmoid`, the `softmax` normalizes each nodes' output value using the sum of all unnormalized nodes). \n",
    "\n",
    "$$ h(\\textbf{z})_i = \\frac{e^{z_i}}{\\sum_{k=1}^C{e^{z_k}}} \\text{,  where  } \\textbf{z} = \\textbf{W} \\times \\textbf{x}  + \\textbf{b} $$\n",
    "\n",
    "In this tutorials, we will use multinomial LR for classifying the MNIST digits (0-9) using 10 output nodes (1 for each of our output classes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below summarizes the model in the context of the MNIST data.\n",
    "\n",
    "![mnist-LR](https://www.cntk.ai/jup/cntk103b_MNIST_LR.png)\n",
    "\n",
    "The goal of training is to find the values of **W** and **b** parameters that fit the training samples and generalize well to the samples outside of the training set.\n",
    "\n",
    "The trainer strives to reduce the `loss` function - a function that expresses the \"difference\" between model predictions and training ground-truth labels - by different optimization approaches, [Stochastic Gradient Descent][] (`sgd`) being one of the most popular one. Typically, one would start with random initialization of the model parameters. The `sgd` optimizer uses [gradient-decent][] to generate a new set of the model parameters in each iteration. \n",
    "\n",
    "The \"classical\" stochastic gradient descent uses a single observation to calculate gradient estimates and update the model parameters. This approach is  attractive since it does not require the entire data set (all observation) to be loaded in memory and also requires gradient computation over fewer datapoints, thus allowing for training on large data sets. However, the updates generated using a single observation sample at a time can vary wildly between iterations. An intermediate ground is to use a small set of observations and use an average of the `loss` or error from that set to update the model parameters. This subset is called a *minibatch*.\n",
    "\n",
    "With minibatches, we often sample observation from the larger training dataset. We repeat the process of model parameters update using different combination of training samples and over a period of time minimize the `loss` (and the error). When the incremental error rates are no longer changing significantly or after a preset number of maximum minibatches to train, we claim that our model is trained.\n",
    "\n",
    "One of the key optimization parameter is called the `learning_rate`. For now, we can think of it as a scaling factor that modulates how much we change the parameters in any iteration. \n",
    "\n",
    "[optimization]: https://en.wikipedia.org/wiki/Category:Convex_optimization\n",
    "[Stochastic Gradient Descent]: https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "[gradient-decent]: http://www.statisticsviews.com/details/feature/5722691/Getting-to-the-Bottom-of-Regression-with-Gradient-Descent.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "nbpresent": {
     "id": "138d1a78-02e2-4bd6-a20e-07b83f303563"
    }
   },
   "outputs": [],
   "source": [
    "# Import the relevant components\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "from cntk.logging.progress_print import ProgressPrinter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "\n",
    "In this tutorial we are using the MNIST data pre-processed to follow CNTK CTF format. The dataset has 50,000 training images, 10,000 validation images, and 10,000 test images with each image being 28 x 28 pixels. Thus the number of features is equal to 784 (= 28 x 28 pixels), 1 per pixel. \n",
    "\n",
    "The data is in the following format:\n",
    "\n",
    "    |labels 0 0 0 0 0 0 0 1 0 0 |features 0 0 0 0 ... \n",
    "                                                  (784 integers each representing a pixel)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we always get the same amount of randomness\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    labelStream = C.io.StreamDef(field='labels', shape=num_label_classes)\n",
    "    featureStream = C.io.StreamDef(field='features', shape=input_dim)\n",
    "    deserializer = C.io.CTFDeserializer(path, C.io.StreamDefs(labels = labelStream, features = featureStream))\n",
    "    return C.io.MinibatchSource(deserializer,\n",
    "       randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition and training\n",
    "In CNTK, a computational network (e.g. a neural network) is a **function object**. On one hand a computational network in CNTK is just a function that you can call to apply to data. On the other hand, a computational network contains learnable parameters that can be accessed like object members. Complicated networks can be composed as hierarchies of simpler ones, which, for example, represent layers. \n",
    "\n",
    "CNTK function objects are represented internally as graph structures in C++ that encode the computation. This graph structure is wraped in the Python class `Function` that exposes the necessary interface so that other Python functions can call it and access its members (such as learnable parameters)\n",
    "\n",
    "The function object is CNTK's single abstraction used to represent different operations from simple **basic operations** without learnable parameters to  **layers**, **recurrent step functions**, **complete models**, **criterion functions** and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data dimensions\n",
    "input_dim = 784\n",
    "num_output_classes = 10\n",
    "\n",
    "# Create inputs for features and labels\n",
    "features = C.input_variable(input_dim)/255\n",
    "labels = C.input_variable(num_output_classes, is_sparse=True)\n",
    "\n",
    "# Define parameters\n",
    "W = C.parameter(shape=(input_dim, num_output_classes))\n",
    "b = C.parameter(shape=(num_output_classes))\n",
    "\n",
    "# And the network\n",
    "z = C.times(features, W) + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the criterion function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = C.cross_entropy_with_softmax(z, labels)\n",
    "metric = C.classification_error(z, labels)\n",
    "criterion = C.combine([loss, metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Train the model using the SGD learner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per minibatch: 0.2\n",
      "Finished Epoch[1]: loss = 0.773105 * 6400, metric = 19.03% * 6400 1.162s (5507.7 samples/s);\n",
      "Finished Epoch[2]: loss = 0.461877 * 6400, metric = 12.06% * 6400 0.115s (55652.2 samples/s);\n",
      "Finished Epoch[3]: loss = 0.380197 * 6400, metric = 10.44% * 6400 0.187s (34224.6 samples/s);\n",
      "Finished Epoch[4]: loss = 0.389870 * 6400, metric = 11.11% * 6400 0.116s (55172.4 samples/s);\n",
      "Finished Epoch[5]: loss = 0.344490 * 6400, metric = 9.73% * 6400 0.123s (52032.5 samples/s);\n",
      "Finished Epoch[6]: loss = 0.363797 * 6400, metric = 10.62% * 6400 0.138s (46376.8 samples/s);\n",
      "Finished Epoch[7]: loss = 0.359560 * 6400, metric = 9.97% * 6400 0.114s (56140.4 samples/s);\n",
      "Finished Epoch[8]: loss = 0.340928 * 6400, metric = 9.69% * 6400 0.144s (44444.4 samples/s);\n",
      "Finished Epoch[9]: loss = 0.324644 * 6400, metric = 8.83% * 6400 0.285s (22456.1 samples/s);\n",
      "Finished Epoch[10]: loss = 0.330513 * 6400, metric = 9.34% * 6400 0.199s (32160.8 samples/s);\n",
      "Finished Epoch[11]: loss = 0.318310 * 6400, metric = 9.16% * 6400 0.194s (32989.7 samples/s);\n",
      "Finished Epoch[12]: loss = 0.299671 * 6400, metric = 8.34% * 6400 0.115s (55652.2 samples/s);\n",
      "Finished Epoch[13]: loss = 0.320537 * 6400, metric = 9.00% * 6400 0.117s (54700.9 samples/s);\n",
      "Finished Epoch[14]: loss = 0.322201 * 6400, metric = 9.08% * 6400 0.163s (39263.8 samples/s);\n",
      "Finished Epoch[15]: loss = 0.314969 * 6400, metric = 8.67% * 6400 0.113s (56637.2 samples/s);\n",
      "Finished Epoch[16]: loss = 0.299890 * 6400, metric = 8.88% * 6400 0.140s (45714.3 samples/s);\n",
      "Finished Epoch[17]: loss = 0.293044 * 6400, metric = 7.92% * 6400 0.123s (52032.5 samples/s);\n",
      "Finished Epoch[18]: loss = 0.302812 * 6400, metric = 8.75% * 6400 0.141s (45390.1 samples/s);\n",
      "Finished Epoch[19]: loss = 0.291931 * 6400, metric = 8.27% * 6400 0.118s (54237.3 samples/s);\n",
      "Finished Epoch[20]: loss = 0.308470 * 6400, metric = 8.41% * 6400 0.136s (47058.8 samples/s);\n",
      "Finished Epoch[21]: loss = 0.311128 * 6400, metric = 8.89% * 6400 0.120s (53333.3 samples/s);\n",
      "Finished Epoch[22]: loss = 0.299958 * 6400, metric = 8.59% * 6400 0.150s (42666.7 samples/s);\n",
      "Finished Epoch[23]: loss = 0.294825 * 6400, metric = 7.97% * 6400 0.121s (52892.6 samples/s);\n",
      "Finished Epoch[24]: loss = 0.298056 * 6400, metric = 8.45% * 6400 0.156s (41025.6 samples/s);\n",
      "Finished Epoch[25]: loss = 0.272368 * 6400, metric = 8.06% * 6400 0.135s (47407.4 samples/s);\n",
      "Finished Epoch[26]: loss = 0.288159 * 6400, metric = 7.80% * 6400 0.124s (51612.9 samples/s);\n",
      "Finished Epoch[27]: loss = 0.304322 * 6400, metric = 7.97% * 6400 0.119s (53781.5 samples/s);\n",
      "Finished Epoch[28]: loss = 0.295931 * 6400, metric = 8.55% * 6400 0.139s (46043.2 samples/s);\n",
      "Finished Epoch[29]: loss = 0.301067 * 6400, metric = 8.38% * 6400 0.117s (54700.9 samples/s);\n",
      "Finished Epoch[30]: loss = 0.290961 * 6400, metric = 8.28% * 6400 0.134s (47761.2 samples/s);\n",
      "Finished Epoch[31]: loss = 0.283343 * 6400, metric = 7.83% * 6400 0.118s (54237.3 samples/s);\n",
      "Finished Epoch[32]: loss = 0.285112 * 6400, metric = 8.03% * 6400 0.143s (44755.2 samples/s);\n",
      "Finished Epoch[33]: loss = 0.276470 * 6400, metric = 7.77% * 6400 0.121s (52892.6 samples/s);\n",
      "Finished Epoch[34]: loss = 0.285783 * 6400, metric = 8.36% * 6400 0.136s (47058.8 samples/s);\n",
      "Finished Epoch[35]: loss = 0.294690 * 6400, metric = 8.16% * 6400 0.121s (52892.6 samples/s);\n",
      "Finished Epoch[36]: loss = 0.266612 * 6400, metric = 7.38% * 6400 0.138s (46376.8 samples/s);\n",
      "Finished Epoch[37]: loss = 0.281170 * 6400, metric = 8.11% * 6400 0.123s (52032.5 samples/s);\n",
      "Finished Epoch[38]: loss = 0.281648 * 6400, metric = 7.56% * 6400 0.139s (46043.2 samples/s);\n",
      "Finished Epoch[39]: loss = 0.301099 * 6400, metric = 8.53% * 6400 0.120s (53333.3 samples/s);\n",
      "Finished Epoch[40]: loss = 0.266258 * 6400, metric = 7.30% * 6400 0.140s (45714.3 samples/s);\n",
      "Finished Epoch[41]: loss = 0.278865 * 6400, metric = 7.81% * 6400 0.121s (52892.6 samples/s);\n",
      "Finished Epoch[42]: loss = 0.281765 * 6400, metric = 7.72% * 6400 0.134s (47761.2 samples/s);\n",
      "Finished Epoch[43]: loss = 0.269162 * 6400, metric = 7.52% * 6400 0.116s (55172.4 samples/s);\n",
      "Finished Epoch[44]: loss = 0.281738 * 6400, metric = 7.89% * 6400 0.138s (46376.8 samples/s);\n",
      "Finished Epoch[45]: loss = 0.287398 * 6400, metric = 7.86% * 6400 0.123s (52032.5 samples/s);\n",
      "Finished Epoch[46]: loss = 0.273597 * 6400, metric = 7.77% * 6400 0.180s (35555.6 samples/s);\n",
      "Finished Epoch[47]: loss = 0.286974 * 6400, metric = 7.95% * 6400 0.244s (26229.5 samples/s);\n",
      "Finished Epoch[48]: loss = 0.273186 * 6400, metric = 7.62% * 6400 0.191s (33507.9 samples/s);\n",
      "Finished Epoch[49]: loss = 0.279456 * 6400, metric = 7.78% * 6400 0.181s (35359.1 samples/s);\n",
      "Finished Epoch[50]: loss = 0.283296 * 6400, metric = 7.69% * 6400 0.112s (57142.9 samples/s);\n",
      "Finished Epoch[51]: loss = 0.271296 * 6400, metric = 7.39% * 6400 0.111s (57657.7 samples/s);\n",
      "Finished Epoch[52]: loss = 0.277610 * 6400, metric = 7.84% * 6400 0.155s (41290.3 samples/s);\n",
      "Finished Epoch[53]: loss = 0.270328 * 6400, metric = 7.62% * 6400 0.116s (55172.4 samples/s);\n",
      "Finished Epoch[54]: loss = 0.276786 * 6400, metric = 7.97% * 6400 0.141s (45390.1 samples/s);\n",
      "Finished Epoch[55]: loss = 0.269292 * 6400, metric = 7.33% * 6400 0.119s (53781.5 samples/s);\n",
      "Finished Epoch[56]: loss = 0.265880 * 6400, metric = 7.78% * 6400 0.136s (47058.8 samples/s);\n",
      "Finished Epoch[57]: loss = 0.272612 * 6400, metric = 7.52% * 6400 0.110s (58181.8 samples/s);\n",
      "Finished Epoch[58]: loss = 0.277559 * 6400, metric = 7.58% * 6400 0.112s (57142.9 samples/s);\n",
      "Finished Epoch[59]: loss = 0.273553 * 6400, metric = 7.42% * 6400 0.139s (46043.2 samples/s);\n",
      "Finished Epoch[60]: loss = 0.278683 * 6400, metric = 7.97% * 6400 0.119s (53781.5 samples/s);\n",
      "Finished Epoch[61]: loss = 0.278005 * 6400, metric = 7.88% * 6400 0.144s (44444.4 samples/s);\n",
      "Finished Epoch[62]: loss = 0.260530 * 6400, metric = 7.27% * 6400 0.120s (53333.3 samples/s);\n",
      "Finished Epoch[63]: loss = 0.264243 * 6400, metric = 7.39% * 6400 0.118s (54237.3 samples/s);\n",
      "Finished Epoch[64]: loss = 0.263594 * 6400, metric = 7.39% * 6400 0.140s (45714.3 samples/s);\n",
      "Finished Epoch[65]: loss = 0.266382 * 6400, metric = 7.30% * 6400 0.116s (55172.4 samples/s);\n",
      "Finished Epoch[66]: loss = 0.263940 * 6400, metric = 7.16% * 6400 0.146s (43835.6 samples/s);\n",
      "Finished Epoch[67]: loss = 0.279589 * 6400, metric = 7.53% * 6400 0.116s (55172.4 samples/s);\n",
      "Finished Epoch[68]: loss = 0.272439 * 6400, metric = 7.97% * 6400 0.136s (47058.8 samples/s);\n",
      "Finished Epoch[69]: loss = 0.253575 * 6400, metric = 7.14% * 6400 0.119s (53781.5 samples/s);\n",
      "Finished Epoch[70]: loss = 0.276635 * 6400, metric = 7.70% * 6400 0.118s (54237.3 samples/s);\n"
     ]
    }
   ],
   "source": [
    "learner = C.sgd(z.parameters, C.learning_rate_schedule(0.2, C.UnitType.minibatch))\n",
    "\n",
    "progress_writer = ProgressPrinter()\n",
    "\n",
    "# Create the reader to the training data set\n",
    "train_file = \"../Data/MNIST_train.txt\"\n",
    "reader_train = create_reader(train_file, True, input_dim, num_output_classes)\n",
    "\n",
    "progress = criterion.train(minibatch_source = reader_train,\n",
    "                    streams = (reader_train.streams.features, reader_train.streams.labels),\n",
    "                    minibatch_size = 64,\n",
    "                    epoch_size = 6400,\n",
    "                    max_epochs = 70,\n",
    "                    parameter_learners=[learner],\n",
    "                    callbacks = [progress_writer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Evaluation [1]: Minibatch[1-157]: metric = 8.11% * 10000;\n"
     ]
    }
   ],
   "source": [
    "validation_file = \"../Data/MNIST_validate.txt\"\n",
    "reader_validate = create_reader(validation_file, False, input_dim, num_output_classes)\n",
    "\n",
    "validation_metric = criterion.test(minibatch_source = reader_validate,\n",
    "                                  minibatch_size = 64,\n",
    "                                  streams = (reader_validate.streams.features, reader_validate.streams.labels),\n",
    "                                  callbacks = [progress_writer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon\n",
    "\n",
    "Try to improve the performance of the model. \n",
    "\n",
    "Hints:\n",
    "- Play with the learning rate, minibatch size and the number of epochs\n",
    "- You can look at regularization - check `l1_regularization` and `l2_regularization` hyper parameters of the `sgd` learner\n",
    "\n",
    "## Final testing\n",
    "\n",
    "\n",
    "DON'T CHEAT. DON'T USE MNIST_test.txt FOR MODEL TRAINING AND SELECTION. DON'T EXECUTE THE BELOW CELL TILL YOU ARE READY FOR THE FINAL TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Evaluation [2]: Minibatch[1-157]: metric = 7.74% * 10000;\n"
     ]
    }
   ],
   "source": [
    "test_file = '../Data/MNIST_test.txt'\n",
    "reader_test = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "test_metric = criterion.test(minibatch_source = reader_test,\n",
    "                           minibatch_size = 64,\n",
    "                           streams = (reader_test.streams.features, reader_test.streams.labels),\n",
    "                           callbacks = [progress_writer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
