{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29b9bd1d-766f-4422-ad96-de0accc1ce58"
    }
   },
   "source": [
    "# Lab 2 - Logistic Regression with MNIST\n",
    "\n",
    "\n",
    "# Model Overview\n",
    "In this tutorial we will build and train a Multiclass Logistic Regression model using the MNIST data. \n",
    "\n",
    "The MNIST data comprises of hand-written digits with little background noise making it a standard dataset to create, experiment and learn deep learning models with reasonably small comptuing resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\", width=200, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) (LR) is a fundamental machine learning technique that uses a linear weighted combination of features and generates probability-based predictions of different classes.  \n",
    "â€‹\n",
    "There are two basic forms of LR: **Binary LR** (with a single output that can predict two classes) and **multinomial LR** (with multiple outputs, each of which is used to predict a single class).  \n",
    "\n",
    "![LR-forms](http://www.cntk.ai/jup/cntk103b_TwoFormsOfLR-v3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Binary Logistic Regression** (see top of figure above), the input features are each scaled by an associated weight and summed together.  The sum is passed through a squashing (aka activation) function and generates an output in [0,1].  This output value (which can be thought of as a probability) is then compared with a threshold (such as 0.5) to produce a binary label (0 or 1).  This technique supports only classification problems with two output classes, hence the name binary LR.  In the binary LR example shown above, the [sigmoid][] function is used as the squashing function.\n",
    "\n",
    "[sigmoid]: https://en.wikipedia.org/wiki/Sigmoid_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Multiclass Linear Regression** (see bottom of figure above), 2 or more output nodes are used, one for each output class to be predicted.  Each summation node uses its own set of weights to scale the input features and sum them together. Instead of passing the summed output of the weighted input features through a sigmoid squashing function, the output is often passed through a `softmax` function (which in addition to squashing, like the `sigmoid`, the `softmax` normalizes each nodes' output value using the sum of all unnormalized nodes). \n",
    "\n",
    "$$ h(\\textbf{z})_i = \\frac{e^{z_i}}{\\sum_{k=1}^C{e^{z_k}}} \\text{,  where  } \\textbf{z} = \\textbf{W} \\times \\textbf{x}  + \\textbf{b} $$\n",
    "\n",
    "In this tutorials, we will use multinomial LR for classifying the MNIST digits (0-9) using 10 output nodes (1 for each of our output classes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below summarizes the model in the context of the MNIST data.\n",
    "\n",
    "![mnist-LR](https://www.cntk.ai/jup/cntk103b_MNIST_LR.png)\n",
    "\n",
    "The goal of training is to find the values of **W** and **b** parameters that fit the training samples and generalize well to the samples outside of the training set.\n",
    "\n",
    "The trainer strives to reduce the `loss` function - a function that expresses the \"difference\" between model predictions and training ground-truth labels - by different optimization approaches, [Stochastic Gradient Descent][] (`sgd`) being one of the most popular one. Typically, one would start with random initialization of the model parameters. The `sgd` optimizer uses [gradient-decent][] to generate a new set of the model parameters in each iteration. \n",
    "\n",
    "The \"classical\" stochastic gradient descent uses a single observation to calculate gradient estimates and update the model parameters. This approach is  attractive since it does not require the entire data set (all observation) to be loaded in memory and also requires gradient computation over fewer datapoints, thus allowing for training on large data sets. However, the updates generated using a single observation sample at a time can vary wildly between iterations. An intermediate ground is to use a small set of observations and use an average of the `loss` or error from that set to update the model parameters. This subset is called a *minibatch*.\n",
    "\n",
    "With minibatches, we often sample observation from the larger training dataset. We repeat the process of model parameters update using different combination of training samples and over a period of time minimize the `loss` (and the error). When the incremental error rates are no longer changing significantly or after a preset number of maximum minibatches to train, we claim that our model is trained.\n",
    "\n",
    "One of the key optimization parameter is called the `learning_rate`. For now, we can think of it as a scaling factor that modulates how much we change the parameters in any iteration. \n",
    "\n",
    "[optimization]: https://en.wikipedia.org/wiki/Category:Convex_optimization\n",
    "[Stochastic Gradient Descent]: https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "[gradient-decent]: http://www.statisticsviews.com/details/feature/5722691/Getting-to-the-Bottom-of-Regression-with-Gradient-Descent.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "138d1a78-02e2-4bd6-a20e-07b83f303563"
    }
   },
   "outputs": [],
   "source": [
    "# Import the relevant components\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "from cntk.logging.progress_print import ProgressPrinter\n",
    "\n",
    "\n",
    "# Select the right target device \n",
    "# C.device.try_set_default_device(C.device.cpu())\n",
    "# C.device.try_set_default_device(C.device.gpu(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "\n",
    "In this tutorial we are using the MNIST data pre-processed to follow CNTK CTF format. The dataset has 50,000 training images, 10,000 validation images, and 10,000 test images with each image being 28 x 28 pixels. Thus the number of features is equal to 784 (= 28 x 28 pixels), 1 per pixel. The variable `num_output_classes` is set to 10 corresponding to the number of digits (0-9) in the dataset.\n",
    "\n",
    "The data is in the following format:\n",
    "\n",
    "    |labels 0 0 0 0 0 0 0 1 0 0 |features 0 0 0 0 ... \n",
    "                                                  (784 integers each representing a pixel)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure we always get the same amount of randomness\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    labelStream = C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False)\n",
    "    featureStream = C.io.StreamDef(field='features', shape=input_dim, is_sparse=False)\n",
    "    deserializer = C.io.CTFDeserializer(path, C.io.StreamDefs(labels = labelStream, features = featureStream))\n",
    "    return C.io.MinibatchSource(deserializer,\n",
    "       randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a computational network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a computational network for multi-class logistic regression\n",
    "def create_mlr_model(features, output_dim):\n",
    "    input_dim = features.shape[0]\n",
    "    weight_param = C.parameter(shape=(input_dim, output_dim))\n",
    "    bias_param = C.parameter(shape=(output_dim))\n",
    "    return C.times(features, weight_param) + bias_param\n",
    "\n",
    "# Define the data dimensions\n",
    "input_dim = 784\n",
    "num_output_classes = 10\n",
    "\n",
    "# Create inputs for features and labels\n",
    "features = C.input(input_dim)\n",
    "labels = C.input(num_output_classes)\n",
    "\n",
    "# Create the MLR model while scaling the input to 0-1 range by dividing each pixel by 255.\n",
    "z = create_mlr_model(features/255.0, num_output_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a trainer using the SGD learner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a trainer using a given reader and the SGD learner \n",
    "def train_model_with_SGD(model, features, labels, reader, num_samples_per_sweep, num_sweeps):\n",
    " \n",
    "    # Define loss and error functions\n",
    "    loss = C.cross_entropy_with_softmax(model, labels)\n",
    "    error = C.classification_error(model, labels)\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    learning_rate = 0.2\n",
    "    lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "    learner = C.sgd(model.parameters, lr_schedule)\n",
    "    progress_printer = ProgressPrinter(500)\n",
    "    trainer = C.Trainer(model, (loss, error), [learner], [progress_printer])\n",
    "\n",
    "   # Initialize the parameters for the trainer\n",
    "    minibatch_size = 64\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps) / minibatch_size\n",
    "\n",
    "       # Map the data streams to the input and labels.\n",
    "    input_map = {\n",
    "        labels  : reader.streams.labels,\n",
    "        features  : reader.streams.features\n",
    "    } \n",
    "\n",
    "    # Run the trainer on and perform model training\n",
    "    start_time = time.time()\n",
    "    for i in range(0, int(num_minibatches_to_train)):\n",
    "        data = reader.next_minibatch(minibatch_size, input_map = input_map)\n",
    "        trainer.train_minibatch(data)\n",
    "\n",
    "    print(time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the reader to the training data set\n",
    "train_file = \"../../Data/MNIST_train.txt\"\n",
    "reader = create_reader(train_file, True, input_dim, num_output_classes)\n",
    "num_samples_per_sweep = 50000\n",
    "num_sweeps = 10\n",
    "train_model_with_SGD(z, features, labels, reader, num_samples_per_sweep, num_sweeps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "### Define the helper test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the test function \n",
    "def test_model(model, features, labels, reader):\n",
    "    evaluator = C.Evaluator(C.classification_error(model, labels))\n",
    "    input_map = {\n",
    "       features : reader.streams.features,\n",
    "       labels: reader.streams.labels\n",
    "    }\n",
    "    \n",
    "    minibatch_size = 2000\n",
    "    test_result = 0.0\n",
    "    num_minibatches = 0\n",
    "    data = reader.next_minibatch(minibatch_size, input_map = input_map)\n",
    "    while bool(data):\n",
    "        test_result = test_result + evaluator.test_minibatch(data)\n",
    "        num_minibatches += 1\n",
    "        data = reader.next_minibatch(minibatch_size, input_map = input_map)\n",
    "    return None if num_minibatches == 0 else test_result*100 / num_minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_file = \"../../Data/MNIST_validate.txt\"\n",
    "reader = create_reader(validation_file, False, input_dim, num_output_classes)\n",
    "error_rate = test_model(z, features, labels, reader)\n",
    "print(\"Average validation error: {0:.2f}%\".format(error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon\n",
    "\n",
    "Try to improve the performance of the model. \n",
    "\n",
    "Hints:\n",
    "- Play with the learning rate, minibatch size and the number of sweeps\n",
    "- You can look at regularization - check `l1_regularization` and `l2_regularization` hyper parameters of the `sgd` learner\n",
    "\n",
    "## Final testing\n",
    "\n",
    "\n",
    "DON'T CHEAT. DON'T USE MNIST_test.txt FOR MODEL TRAINING AND SELECTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_file = '../../Data/MNIST_test.txt'\n",
    "reader = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "error_rate = test_model(z, features, labels, reader)\n",
    "print(\"Average test error: {0:.2f}%\".format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
