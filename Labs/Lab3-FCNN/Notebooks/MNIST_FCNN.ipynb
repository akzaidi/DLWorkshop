{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29b9bd1d-766f-4422-ad96-de0accc1ce58"
    }
   },
   "source": [
    "# Lab 3 - Fully Connected Feedforward Network with MNIST\n",
    "# Model Overview\n",
    "\n",
    "In this lab, we will train a fully connected feedforward network on MNIST data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\", width=200, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our fully connected feedforward network - a.k.a multi-layer perceptron - will be relatively simple with 2 hidden layers (`num_hidden_layers`). The number of nodes in the hidden layer being a parameter specified by `hidden_layers_dim`. The figure below illustrates the entire model we will use in this tutorial in the context of MNIST data.\n",
    "\n",
    "![model-mlp](http://cntk.ai/jup/cntk103c_MNIST_MLP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Walkthrough\n",
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "138d1a78-02e2-4bd6-a20e-07b83f303563"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "from cntk.logging.progress_print import ProgressPrinter\n",
    "\n",
    "# Select the right target device when this notebook is being tested:\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        C.device.try_set_default_device(C.device.cpu())\n",
    "    else:\n",
    "        C.device.try_set_default_device(C.device.gpu(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "\n",
    "In this tutorial we are using the MNIST data. The dataset has 50,000 training images, 10 validation images and 10,000 test images with each image being 28 x 28 pixels. Thus the number of features is equal to 784 (= 28 x 28 pixels), 1 per pixel. The variable `num_output_classes` is set to 10 corresponding to the number of digits (0-9) in the dataset.\n",
    "\n",
    "The data is in the following format:\n",
    "\n",
    "    |labels 0 0 0 0 0 0 0 1 0 0 |features 0 0 0 0 ... \n",
    "                                                  (784 integers each representing a pixel)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure we always get the same amount of randomness\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        labels = C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n",
    "        features   = C.io.StreamDef(field='features', shape=input_dim, is_sparse=False)\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "### Set up a computational network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a fully connected feedforward classification network with sigmoid neurons in the hidden layers\n",
    "def create_fcnn_model(features, num_hidden_layers, hidden_layers_dim, num_output_classes):\n",
    "    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.ops.sigmoid):\n",
    "        h = features\n",
    "        for _ in range(num_hidden_layers):\n",
    "            h = C.layers.Dense(hidden_layers_dim)(h)\n",
    "        r = C.layers.Dense(num_output_classes, activation = None)(h)\n",
    "        return r\n",
    "\n",
    "    \n",
    "# Configure a two hidden-layer FCN with softmax output and cross-entropy loss\n",
    "input_dim = 784\n",
    "num_hidden_layers = 2\n",
    "hidden_layers_dim = 400\n",
    "num_output_classes = 10\n",
    "\n",
    "features = C.input(input_dim)\n",
    "labels = C.input(num_output_classes)\n",
    "\n",
    "z = create_fcnn_model(features/255.0, num_hidden_layers, hidden_layers_dim, num_output_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a trainer using the SGD learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a trainer using a given reader and the SGD learner \n",
    "def train_model_with_SGD(model, features, labels, reader, num_samples_per_sweep, num_sweeps):\n",
    " \n",
    "    # Define loss and error functions\n",
    "    loss = C.cross_entropy_with_softmax(model, labels)\n",
    "    error = C.classification_error(model, labels)\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    learning_rate = 0.2\n",
    "    lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "    learner = C.sgd(model.parameters, lr_schedule)\n",
    "    progress_printer = ProgressPrinter(500)\n",
    "    trainer = C.Trainer(model, (loss, error), [learner], [progress_printer])\n",
    "\n",
    "   # Initialize the parameters for the trainer\n",
    "    minibatch_size = 64\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps) / minibatch_size\n",
    "\n",
    "       # Map the data streams to the input and labels.\n",
    "    input_map = {\n",
    "        labels  : reader.streams.labels,\n",
    "        features  : reader.streams.features\n",
    "    } \n",
    "\n",
    "    # Run the trainer on and perform model training\n",
    "    start_time = time.time()\n",
    "    for i in range(0, int(num_minibatches_to_train)):\n",
    "        data = reader.next_minibatch(minibatch_size, input_map = input_map)\n",
    "        trainer.train_minibatch(data)\n",
    "\n",
    "    print(time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the reader to the training data set\n",
    "train_file = \"../../Data/MNIST_train.txt\"\n",
    "reader = create_reader(train_file, True, input_dim, num_output_classes)\n",
    "num_samples_per_sweep = 50000\n",
    "num_sweeps = 10\n",
    "train_model_with_SGD(z, features, labels, reader, num_samples_per_sweep, num_sweeps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "## Define the helper test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the test function \n",
    "def test_model(model, features, labels, reader):\n",
    "    evaluator = C.Evaluator(C.classification_error(model, labels))\n",
    "    input_map = {\n",
    "       features : reader.streams.features,\n",
    "       labels: reader.streams.labels\n",
    "    }\n",
    "    \n",
    "    minibatch_size = 2000\n",
    "    test_result = 0.0\n",
    "    num_minibatches = 0\n",
    "    data = reader.next_minibatch(minibatch_size, input_map = input_map)\n",
    "    while bool(data):\n",
    "        test_result = test_result + evaluator.test_minibatch(data)\n",
    "        num_minibatches += 1\n",
    "        data = reader.next_minibatch(minibatch_size, input_map = input_map)\n",
    "    return None if num_minibatches == 0 else test_result*100 / num_minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_file = \"../../Data/MNIST_validate.txt\"\n",
    "reader = create_reader(validation_file, False, input_dim, num_output_classes)\n",
    "error_rate = test_model(z, features, labels, reader)\n",
    "print(\"Average validation error: {0:.2f}%\".format(error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hackathon\n",
    "\n",
    "Try to improve the performance of the model. \n",
    "\n",
    "Hints:\n",
    "- Try different activation functions in hidden layers\n",
    "- Play with the learning rate, minibatch size and the number of sweeps\n",
    "- You can look at regularization - check `l1_regularization` and `l2_regularization` hyper parameters of the `sgd` learner\n",
    "- Try different optimization algorithms\n",
    "\n",
    "## Final testing\n",
    "\n",
    "\n",
    "DON'T CHEAT. DON'T USE MNIST_test.txt FOR MODEL TRAINING AND SELECTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_file = '../../Data/MNIST_test.txt'\n",
    "reader = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "error_rate = test_model(z, features, labels, reader)\n",
    "print(\"Average test error: {0:.2f}%\".format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
