{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Convolutional Neural Network with MNIST\n",
    "\n",
    "\n",
    "# Model Overview\n",
    "\n",
    "In this lab we will train a Convolutional Neural Network (CNN) on MNIST data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "Image(url= \"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\", width=200, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29b9bd1d-766f-4422-ad96-de0accc1ce58"
    }
   },
   "source": [
    "\n",
    "## Overview of convolutional neural networks\n",
    "A [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN, or ConvNet) is a type of [feed-forward](https://en.wikipedia.org/wiki/Feedforward_neural_network) artificial neural network made up of neurons that have learnable weights and biases. The CNNs take advantage of the spatial nature of the data. In nature, we perceive different objects by their shapes, size and colors. For example, objects in a natural scene are typically edges, corners/vertices (defined by two of more edges), color patches etc. These primitives are often identified using different detectors (e.g., edge detection, color detector) or combination of detectors interacting to facilitate image interpretation (object classification, region of interest detection, scene description etc.) in real world vision related tasks. These detectors are also known as filters. Convolution is a mathematical operator that takes an image and a filter as input and produces a filtered output (representing say egdges, corners, colors etc in the input image).  Historically, these filters are a set of weights that were often hand crafted or modeled with mathematical functions (e.g., [Gaussian](https://en.wikipedia.org/wiki/Gaussian_filter) / [Laplacian](http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm) / [Canny](https://en.wikipedia.org/wiki/Canny_edge_detector) filter).  The filter outputs are mapped through non-linear activation functions mimicking human brain cells called [neurons](https://en.wikipedia.org/wiki/Neuron).\n",
    "\n",
    "Convolutional networks provide a machinery to learn these filters from the data directly instead of explicit mathematical models and have been found to be superior (in real world tasks) compared to historically crafted filters.  With convolutional networks, the focus is on learning the filter weights instead of learning individually fully connected pair-wise (between inputs and outputs) weights. In this way, the number of weights to learn is reduced when compared with the traditional MLP networks from the previous tutorials.  In a convolutional network, one learns several filters ranging from few single digits to few thousands depending on the network complexity.\n",
    "\n",
    "Many of the CNN primitives have been shown to have a conceptually parallel components in brain's [visual cortex](https://en.wikipedia.org/wiki/Visual_cortex). The group of neurons cells in visual cortex emit responses when stimulated. This region is known as the receptive field (RF). Equivalently, in convolution the input region corresponding to the filter dimensions can be considered as the receptive field. Popular deep CNNs or ConvNets (such as [AlexNet](https://en.wikipedia.org/wiki/AlexNet), [VGG](https://arxiv.org/abs/1409.1556), [Inception](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf), [ResNet](https://arxiv.org/pdf/1512.03385v1.pdf)) that are used for various [computer vision](https://en.wikipedia.org/wiki/Computer_vision) tasks have many of these architectural primitives (inspired from biology).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Model Creation'></a>\n",
    "## CNN Model Creation\n",
    "\n",
    "CNN is a feedforward network made up of bunch of layers in such a way that the output of one layer becomes the input to the next layer (similar to MLP). In MLP, all possible pairs of input pixels are connected to the output nodes with each pair having a weight, thus leading to a combinatorial explosion of parameters to be learnt and also increasing the possibility of overfitting ([details](http://cs231n.github.io/neural-networks-1/)). Convolution layers take advantage of the spatial arrangement of the pixels and learn multiple filters that significantly reduce the amount of parameters in the network ([details](http://cs231n.github.io/convolutional-networks/)). The size of the filter is a parameter of the convolution layer.  \n",
    "\n",
    "In this section, we introduce the basics of convolution operations. We show the illustrations in the context of RGB images (3 channels), eventhough the MNIST data we are using in this tutorial is a grayscale image (single channel).\n",
    "\n",
    "![input-rgb](https://www.cntk.ai/jup/cntk103d_rgb.png)\n",
    "\n",
    "### Convolution Layer\n",
    "\n",
    "A convolution layer is a set of filters. Each filter is defined by a weight (**W**) matrix, and  bias ($b$).\n",
    "\n",
    "![input-filter](https://www.cntk.ai/jup/cntk103d_filterset.png)\n",
    "\n",
    "These filters are scanned across the image performing the dot product between the weights and corresponding input value ($\\vec{x}^T$). The bias value is added to the output of the dot product and the resulting sum is optionally mapped through an activation function. This process is illustrated in the following animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(url=\"https://www.cntk.ai/jup/cntk103d_conv2d_final.gif\", width= 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution layers incorporate following key features:\n",
    "\n",
    "   - Instead of being fully-connected to all pairs of input and output nodes , each convolution node is **locally-connected** to a subset of input nodes localized to a smaller input region, also referred to as receptive field (RF). The figure above illustrates a small 3 x 3 regions in the image as the RF region. In the case of an RGB, image there would be three such 3 x 3 regions, one each of the 3 color channels. \n",
    "   \n",
    "   \n",
    "   - Instead of having a single set of weights (as in a Dense layer), convolutional layers have multiple sets (shown in figure with multiple colors), called **filters**. Each filter detects features within each possible RF in the input image.  The output of the convolution is a set of `n` sub-layers (shown in the animation below) where `n` is the number of filters (refer to the above figure).  \n",
    "   \n",
    "     \n",
    "   - Within a sublayer, instead of each node having its own set of weights, a single set of **shared weights** are used by all nodes in that sublayer. This reduces the number of parameters to be learnt and thus overfitting. This also opens the door for several aspects of deep learning which has enabled very practical solutions to be built:\n",
    "    -- Handling larger images (say 512 x 512)\n",
    "    - Trying larger filter sizes (corresponding to a larger RF) say 11 x 11\n",
    "    - Learning more filters (say 128)\n",
    "    - Explore deeper architectures (100+ layers)\n",
    "    - Achieve translation invariance (the ability to recognize a feature independent of where they appear in the image). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strides and Pad parameters\n",
    "\n",
    "**How are filters positioned?** In general, the filters are arranged in overlapping tiles, from left to right, and top to bottom.  Each convolution layer has a parameter to specify the `filter_shape`, specifying the width and height of the filter in case most natural scene images.  There is a parameter (`strides`) that controls the how far to step to right when moving the filters through multiple RF's in a row, and how far to step down when moving to the next row.  The boolean parameter `pad` controls if the input should be padded around the edges to allow a complete tiling of the RF's near the borders. \n",
    "\n",
    "The animation above shows the results with a `filter_shape` = (3, 3), `strides` = (2, 2) and `pad` = False. The two animations below show the results when `pad` is set to True. First, with a stride of 2 and second having a stride of 1.\n",
    "Note: the shape of the output (the teal layer) is different between the two stride settings. Many a times your decision to pad and the stride values to choose are based on the shape of the output layer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot images with strides of 2 and 1 with padding turned on\n",
    "images = [(\"https://www.cntk.ai/jup/cntk103d_padding_strides.gif\" , 'With stride = 2'),\n",
    "          (\"https://www.cntk.ai/jup/cntk103d_same_padding_no_strides.gif\", 'With stride = 1')]\n",
    "\n",
    "for im in images:\n",
    "    print(im[1])\n",
    "    display(Image(url=im[0], width=200, height=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Walkthrough\n",
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "138d1a78-02e2-4bd6-a20e-07b83f303563"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cntk as C\n",
    "from cntk.logging.progress_print import ProgressPrinter\n",
    "\n",
    "# Select the right target device \n",
    "# C.device.try_set_default_device(C.device.cpu())\n",
    "# C.device.try_set_default_device(C.device.gpu(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "\n",
    "\n",
    "In previous tutorials, as shown below, we have always flattened the input image into a vector.  With convoultional networks, we do not flatten the image in this way.\n",
    "\n",
    "![MNIST-flat](https://www.cntk.ai/jup/cntk103a_MNIST_input.png)\n",
    "\n",
    "**Input Dimensions**:  \n",
    "\n",
    "In convolutional networks for images, the input data is often shaped as a 3D matrix (number of channels, image width, height), which preserves the spatial relationship between the pixels. In the figure above, the MNIST image is a single channel (grayscale) data, so the input dimension is specified as a (1, image width, image height) tuple. \n",
    "\n",
    "![input-rgb](https://www.cntk.ai/jup/cntk103d_rgb.png)\n",
    "\n",
    "Natural scene color images are often presented as Red-Green-Blue (RGB) color channels. The input dimension of such images are specified as a (3, image width, image height) tuple. If one has RGB input data as a volumetric scan with volume width, volume height and volume depth representing the 3 axes, the input data format would be specified by a tuple of 4 values (3, volume width, volume height, volume depth). In this way CNTK enables specification of input images in arbitrary higher-dimensional space.\n",
    "\n",
    "Since our training data is stored on our local machine in the CNTK CTF format,\n",
    "    |labels 0 0 0 1 0 0 0 0 0 0 |features 0 255 0 123 ... \n",
    "                                                  (784 integers each representing a pixel gray level)\n",
    "    \n",
    "we will need to reshape our data into a 3D matrix when defining the input variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure we always get the same amount of randomness\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define a reader for the CTF formatted MNIST files \n",
    "def create_reader(path, is_training, input_dim, label_dim):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        features  = C.io.StreamDef(field='features', shape=input_dim, is_sparse=False),\n",
    "        labels    = C.io.StreamDef(field='labels',   shape=label_dim, is_sparse=False)\n",
    "    )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "## Setup a computational network\n",
    "\n",
    "\n",
    "The first model we build is a simple convolution only network. Here we have two convolutional layers. Since, our task is to detect the 10 digits in the MNIST database, the output of the network should be a vector of length 10, 1 element corresponding to each digit. This is achieved by projecting the output of the last convolutional layer using a dense layer with the output being `num_output_classes`. We have seen this before with Logistic Regression and MLP where features were mapped to the number of classes in the final layer. Also, note that since we will be using the `softmax` operation that is combined with the `cross entropy` loss function during training (see a few cells below), the final dense layer has no activation function associated with it.\n",
    "\n",
    "The following figure illustrates the model we are going to build. Note the parameters in the model below are to be experimented with. These are often called network hyperparameters. Increasing the filter shape leads to an increase in the number of model parameters, increases the compute time and helps the model better fit to the data. However, one runs the risk of [overfitting](https://en.wikipedia.org/wiki/Overfitting). Typically, the number of filters in the deeper layers are more than the number of filters in the layers before them. We have chosen 8, 16 for the first and second layers, respectively. These hyperparameters should be experimented with during model building.\n",
    "\n",
    "![conv-only](https://www.cntk.ai/jup/cntk103d_convonly2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a convolutional neura network with \n",
    "def create_cnn_model(features, num_output_classes):\n",
    "    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.ops.relu):\n",
    "        h = features\n",
    "        h = C.layers.Convolution2D(filter_shape=(5,5),\n",
    "                                   num_filters = 8,\n",
    "                                   strides = (2,2),\n",
    "                                   pad = True,\n",
    "                                   name = 'first_conv')(h)\n",
    "        h = C.layers.Convolution2D(filter_shape = (5,5),\n",
    "                                  num_filters = 16,\n",
    "                                  strides = (2, 2),\n",
    "                                  pad = True,\n",
    "                                  name = 'second_conv')(h)\n",
    "        r = C.layers.Dense(num_output_classes, activation=None, name = 'classify')(h)\n",
    "        return r\n",
    "\n",
    "    # Define MNIST data dimensions\n",
    "input_dim = 784\n",
    "input_dim_model = (1, 28, 28)\n",
    "num_output_classes = 10\n",
    "\n",
    "# Create inputs for features and labels\n",
    "features = C.input(input_dim_model)\n",
    "labels = C.input(num_output_classes)\n",
    "\n",
    "# Create the CNN model while scaling the input to 0-1 range by dividing each pixel by 255.\n",
    "z = create_cnn_model(features/255.0, num_output_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the output shapes / parameters of different components\n",
    "print(\"Output Shape of the first convolution layer:\", z.first_conv.shape)\n",
    "print(\"Bias value of the last dense layer:\", z.classify.b.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding number of model parameters to be estimated is key to deep learning since there is a direct dependency on the amount of data one needs to have. You need more data for a model that has larger number of parameters to prevent overfitting. In other words, with a fixed amount of data, one has to constrain the number of parameters. There is no golden rule between the amount of data one needs for a model. However, there are ways one can boost performance of model training with [data augmentation](https://deeplearningmania.quora.com/The-Power-of-Data-Augmentation-2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of parameters in the network\n",
    "C.logging.log_number_of_parameters(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has 2 convolution layers each having a weight and bias. This adds up to 4 parameter tensors. Additionally the dense layer has weight and bias tensors. Thus, the 6 parameter tensors.\n",
    "\n",
    "Let us now count the number of parameters:\n",
    "- *First convolution layer*: There are 8 filters each of size (1 x 5 x 5) where 1 is the number of channels in the input image. This adds up to 200 values in the weight matrix and 8 bias values.\n",
    "\n",
    "\n",
    "- *Second convolution layer*: There are 16 filters each of size (8 x 5 x 5) where 8 is the number of channels in the input to the second layer (= output of the first layer). This adds up to 3200 values in the weight matrix and 16 bias values.\n",
    "\n",
    "\n",
    "- *Last dense layer*: There are 16 x 7 x 7 input values and it produces 10 output values corresponding to the 10 digits in the MNIST dataset. This corresponds to (16 x 7 x 7) x 10 weight values and 10 bias values.\n",
    "\n",
    "Adding these up gives the 11274 parameters in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a trainer using the SGD learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a trainer using a given reader and the SGD learner \n",
    "def train_model_with_SGD(model, features, labels, reader, num_samples_per_sweep, num_sweeps):\n",
    " \n",
    "    # Define loss and error functions\n",
    "    loss = C.cross_entropy_with_softmax(model, labels)\n",
    "    error = C.classification_error(model, labels)\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    learning_rate = 0.2\n",
    "    lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "    learner = C.sgd(model.parameters, lr_schedule)\n",
    "    progress_printer = ProgressPrinter(500)\n",
    "    trainer = C.Trainer(model, (loss, error), [learner], [progress_printer])\n",
    "\n",
    "   # Initialize the parameters for the trainer\n",
    "    minibatch_size = 64\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps) / minibatch_size\n",
    "\n",
    "       # Map the data streams to the input and labels.\n",
    "    input_map = {\n",
    "        labels  : reader.streams.labels,\n",
    "        features  : reader.streams.features\n",
    "    } \n",
    "\n",
    "    # Run the trainer on and perform model training\n",
    "    start_time = time.time()\n",
    "    for i in range(0, int(num_minibatches_to_train)):\n",
    "        data = reader.next_minibatch(minibatch_size, input_map = input_map)\n",
    "        trainer.train_minibatch(data)\n",
    "\n",
    "    print(time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Run the trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the reader to the training data set\n",
    "train_file = \"../../Data/MNIST_train.txt\"\n",
    "reader = create_reader(train_file, True, input_dim, num_output_classes)\n",
    "num_samples_per_sweep = 50000\n",
    "num_sweeps = 10\n",
    "train_model_with_SGD(z, features, labels, reader, num_samples_per_sweep, num_sweeps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "## Define the helper test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the evaluater function \n",
    "def test_model(model, features, labels, reader):\n",
    "    evaluator = C.Evaluator(C.classification_error(model, labels))\n",
    "    input_map = {\n",
    "       features : reader.streams.features,\n",
    "       labels: reader.streams.labels\n",
    "    }\n",
    "    \n",
    "    minibatch_size = 2000\n",
    "    test_result = 0.0\n",
    "    num_minibatches = 0\n",
    "    data = reader.next_minibatch(minibatch_size, input_map = input_map)\n",
    "    while bool(data):\n",
    "        test_result = test_result + evaluator.test_minibatch(data)\n",
    "        num_minibatches += 1\n",
    "        data = reader.next_minibatch(minibatch_size, input_map = input_map)\n",
    "    return None if num_minibatches == 0 else test_result*100 / num_minibatches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_file = \"../../Data/MNIST_validate.txt\"\n",
    "reader = create_reader(validation_file, False, input_dim, num_output_classes)\n",
    "error_rate = test_model(z, features, labels, reader)\n",
    "print(\"Average validation error: {0:.2f}%\".format(error_rate))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Evolving the model\n",
    "## Pooling Layer\n",
    "\n",
    "Often a times, one needs to control the number of parameters especially when having deep networks. For every layer of the convolution layer output (each layer, corresponds to the output of a filter), one can have a pooling layer. Pooling layers are typically introduced to:\n",
    "- Reduce the dimensionality of the previous layer (speeding up the network),\n",
    "- Makes the model more tolerant to changes in object location in the image. For example, even when a digit is shifted to one side of the image instead of being in the middle, the classifer would perform the classification task well.\n",
    "\n",
    "The calculation on a pooling node is much simpler than a normal feedforward node.  It has no weight, bias, or activation function.  It uses a simple aggregation function (like max or average) to compute its output.  The most commonly used function is \"max\" - a max pooling node simply outputs the maximum of the input values corresponding to the filter position of the input. The figure below shows the input values in a 4 x 4 region. The max pooling window size is 2 x 2 and starts from the top left corner. The maximum value within the window becomes the output of the region. Every time the model is shifted by the amount specified by the stride parameter (as shown in the figure below) and the maximum pooling operation is repeated. \n",
    "![maxppool](https://cntk.ai/jup/201/MaxPooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative is average pooling, which emits that average value instead of the maximum value. The two different pooling opearations are summarized in the animation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot images with strides of 2 and 1 with padding turned on\n",
    "images = [(\"https://www.cntk.ai/jup/c103d_max_pooling.gif\" , 'Max pooling'),\n",
    "          (\"https://www.cntk.ai/jup/c103d_average_pooling.gif\", 'Average pooling')]\n",
    "\n",
    "for im in images:\n",
    "    print(im[1])\n",
    "    display(Image(url=im[0], width=200, height=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical convolution network\n",
    "\n",
    "![mnist-conv-mp](http://www.cntk.ai/jup/conv103d_mnist-conv-mp.png)\n",
    "\n",
    "A typical CNN contains a set of alternating convolution and pooling layers followed by a dense output layer for classification. You will find variants of this structure in many classical deep networks (VGG, AlexNet etc).    \n",
    "\n",
    "The illustrations are presented in the context of 2-dimensional (2D) images, but the concept and the CNTK components can operate on any dimensional data. The above schematic shows 2 convolution layer and 2 max-pooling layers. A typical strategy is to increase the number of filters in the deeper layers while reducing the spatial size of each intermediate layers. intermediate layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical convolutional networks have interlacing convolution and max pool layers. The previous model had only convolution layer. In this section, you will create a model with the following architecture.\n",
    "\n",
    "![conv-only](https://www.cntk.ai/jup/cntk103d_conv_max2.png)\n",
    "\n",
    "You will use the CNTK [MaxPooling](https://cntk.ai/pythondocs/cntk.layers.layers.html#cntk.layers.layers.MaxPooling) function to achieve this task. You will edit the `create_model` function below and add the MaxPooling operation. \n",
    "\n",
    "Hint: We provide the solution a few cells below. Refrain from looking ahead and try to add the layer yourself first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the updated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a convolutional neural network - v2 \n",
    "def create_cnn2_model(features, num_output_classes):\n",
    "    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.ops.relu):\n",
    "        h = features\n",
    "        h = C.layers.Convolution2D(filter_shape=(5,5),\n",
    "                                   num_filters = 8,\n",
    "                                   strides = (2,2),\n",
    "                                   pad = True,\n",
    "                                   name = 'first_conv')(h)\n",
    "        h = C.layers.MaxPooling(filter_shape=(2,2),\n",
    "                               strides=(2,2),\n",
    "                               name = \"first_max\")(h)\n",
    "        h = C.layers.Convolution2D(filter_shape = (5,5),\n",
    "                                  num_filters = 16,\n",
    "                                  strides = (2, 2),\n",
    "                                  pad = True,\n",
    "                                  name = 'second_conv')(h)\n",
    "        h = C.layers.MaxPooling(filter_shape=(3,3),\n",
    "                                strides=(3,3),\n",
    "                                name = 'second_max')(h)\n",
    "        r = C.layers.Dense(num_output_classes, activation=None, name = 'classify')(h)\n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "zv2 = create_cnn2_model(features/255.0, num_output_classes)\n",
    "train_file = \"../../Data/MNIST_train.txt\"\n",
    "reader = create_reader(train_file, True, input_dim, num_output_classes)\n",
    "train_model_with_SGD(zv2, features, labels, reader, num_samples_per_sweep, num_sweeps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "validation_file = \"../../Data/MNIST_validate.txt\"\n",
    "reader = create_reader(validation_file, False, input_dim, num_output_classes)\n",
    "error_rate = test_model(zv2, features, labels, reader)\n",
    "print(\"Average validation error for v2 model: {0:.2f}%\".format(error_rate))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hackathon\n",
    "\n",
    "Try to improve the performance of the model. \n",
    "\n",
    "Hints:\n",
    "- Try different number of feature maps\n",
    "- Try different number of convolutional layers\n",
    "- Try two dense layers in the final classification part of the network\n",
    "\n",
    "## Final testing\n",
    "\n",
    "\n",
    "DON'T CHEAT. DON'T USE MNIST_test.txt FOR MODEL TRAINING AND SELECTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_file = '../../Data/MNIST_test.txt'\n",
    "reader = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "error_rate = test_model(z, features, labels, reader)\n",
    "print(\"Average test error: {0:.2f}%\".format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
